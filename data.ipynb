{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_inputs = [\n",
    "  (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "  (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),\n",
    "  (\"What are reptiles known for?\", \"Having scales\"),\n",
    "  (\"What's the main characteristic of amphibians?\", \"They live both in water and on land\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Elementary Animal Questions\"\n",
    "\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Questions and answers about animal phylogenetics.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_prompt, output_answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs={\"answer\": output_answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "csv_file = 'extended_questions_answers.csv' \n",
    "input_keys = ['Question'] \n",
    "output_keys = ['Answer', 'Explanation', 'Category', 'Difficulty']  \n",
    "dataset = client.upload_csv(\n",
    "    csv_file=csv_file,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"My Extended CSV Dataset\",\n",
    "    description=\"Dataset created from an extended CSV file\",\n",
    "    data_type=\"kv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '6f74176dc9424cffb64f740a136a3112-ChatOpenAI' at:\n",
      "https://smith.langchain.com/projects/p/6235b12d-63fc-4bc9-a60f-63e2219ddc74?eval=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': '6f74176dc9424cffb64f740a136a3112-ChatOpenAI',\n",
       " 'results': <Task pending name='Task-5' coro=<_arun_on_examples() running at c:\\Users\\User\\Desktop\\Langsmith\\langsmithvenv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:816>>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"qa\",\n",
    "        \"context_qa\",\n",
    "        \"cot_qa\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = Client()\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=ChatOpenAI(),\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")\n",
    "eval_llm = ChatOpenAI(temperature=0.0)\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        RunEvalConfig.QA(llm=eval_llm, prompt=PROMPT),\n",
    "        RunEvalConfig.ContextQA(llm=eval_llm),\n",
    "        RunEvalConfig.CoTQA(llm=eval_llm),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '81b1e391dec94a2ca8d92444a0f9a08c-ChatOpenAI' at:\n",
      "https://smith.langchain.com/projects/p/1cd12737-4629-4997-9fc4-c91917ae7678?eval=true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': '81b1e391dec94a2ca8d92444a0f9a08c-ChatOpenAI',\n",
       " 'results': <Task pending name='Task-46' coro=<_arun_on_examples() running at c:\\Users\\User\\Desktop\\Langsmith\\langsmithvenv\\Lib\\site-packages\\langchain\\smith\\evaluation\\runner_utils.py:816>>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=eval_llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmithvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
